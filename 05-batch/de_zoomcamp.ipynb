{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test PySpark is the first execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+--------------------+------------+\n",
      "|LocationID|      Borough|                Zone|service_zone|\n",
      "+----------+-------------+--------------------+------------+\n",
      "|         1|          EWR|      Newark Airport|         EWR|\n",
      "|         2|       Queens|         Jamaica Bay|   Boro Zone|\n",
      "|         3|        Bronx|Allerton/Pelham G...|   Boro Zone|\n",
      "|         4|    Manhattan|       Alphabet City| Yellow Zone|\n",
      "|         5|Staten Island|       Arden Heights|   Boro Zone|\n",
      "|         6|Staten Island|Arrochar/Fort Wad...|   Boro Zone|\n",
      "|         7|       Queens|             Astoria|   Boro Zone|\n",
      "|         8|       Queens|        Astoria Park|   Boro Zone|\n",
      "|         9|       Queens|          Auburndale|   Boro Zone|\n",
      "|        10|       Queens|        Baisley Park|   Boro Zone|\n",
      "|        11|     Brooklyn|          Bath Beach|   Boro Zone|\n",
      "|        12|    Manhattan|        Battery Park| Yellow Zone|\n",
      "|        13|    Manhattan|   Battery Park City| Yellow Zone|\n",
      "|        14|     Brooklyn|           Bay Ridge|   Boro Zone|\n",
      "|        15|       Queens|Bay Terrace/Fort ...|   Boro Zone|\n",
      "|        16|       Queens|             Bayside|   Boro Zone|\n",
      "|        17|     Brooklyn|             Bedford|   Boro Zone|\n",
      "|        18|        Bronx|        Bedford Park|   Boro Zone|\n",
      "|        19|       Queens|           Bellerose|   Boro Zone|\n",
      "|        20|        Bronx|             Belmont|   Boro Zone|\n",
      "+----------+-------------+--------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('test') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# df = spark.read \\\n",
    "#     .option(\"header\", \"true\") \\\n",
    "#     .csv('taxi_zone_lookup.csv')\n",
    "\n",
    "# df.show()\n",
    "# df.write.parquet('zones', mode=\"overwrite\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Homework week5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The PySpark 3.5.3 version is running...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/02 14:27:14 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder.master(\"local[*]\") \\\n",
    "                    .appName('BigData-ETL.com') \\\n",
    "                    .getOrCreate()\n",
    "\n",
    "print(f'The PySpark {spark.version} version is running...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.5.3'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import types\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('test') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.version\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4458.93s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-03-02 14:29:42--  https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-10.parquet\n",
      "Resolving d37ci6vzurychx.cloudfront.net (d37ci6vzurychx.cloudfront.net)... 2600:9000:2795:f000:b:20a5:b140:21, 2600:9000:2795:600:b:20a5:b140:21, 2600:9000:2795:b600:b:20a5:b140:21, ...\n",
      "Connecting to d37ci6vzurychx.cloudfront.net (d37ci6vzurychx.cloudfront.net)|2600:9000:2795:f000:b:20a5:b140:21|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 64346071 (61M) [binary/octet-stream]\n",
      "Saving to: ‘yellow_tripdata_2024-10.parquet’\n",
      "\n",
      "yellow_tripdata_202 100%[===================>]  61,36M  72,6MB/s    in 0,8s    \n",
      "\n",
      "2025-03-02 14:29:43 (72,6 MB/s) - ‘yellow_tripdata_2024-10.parquet’ saved [64346071/64346071]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-10.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('VendorID', IntegerType(), True), StructField('tpep_pickup_datetime', TimestampNTZType(), True), StructField('tpep_dropoff_datetime', TimestampNTZType(), True), StructField('passenger_count', LongType(), True), StructField('trip_distance', DoubleType(), True), StructField('RatecodeID', LongType(), True), StructField('store_and_fwd_flag', StringType(), True), StructField('PULocationID', IntegerType(), True), StructField('DOLocationID', IntegerType(), True), StructField('payment_type', LongType(), True), StructField('fare_amount', DoubleType(), True), StructField('extra', DoubleType(), True), StructField('mta_tax', DoubleType(), True), StructField('tip_amount', DoubleType(), True), StructField('tolls_amount', DoubleType(), True), StructField('improvement_surcharge', DoubleType(), True), StructField('total_amount', DoubleType(), True), StructField('congestion_surcharge', DoubleType(), True), StructField('Airport_fee', DoubleType(), True)])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .parquet('yellow_tripdata_2024-10.parquet')\n",
    "\n",
    "\n",
    "df.schema\n",
    "\n",
    "\n",
    "# df = df.repartition(24)\n",
    "\n",
    "# df.write.parquet('data/pq/yellow_tripdata/2024/10/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "% ls -l *\n",
    "-rw-r--r--  1 user  staff        0 Mar  2 14:45 _SUCCESS\n",
    "-rw-r--r--  1 user  staff  3925811 Mar  2 14:45 part-00000-8c83f3b8-5e89-4fd3-b5da-607f38d15341-c000.snappy.parquet\n",
    "-rw-r--r--  1 user  staff  3925869 Mar  2 14:45 part-00001-8c83f3b8-5e89-4fd3-b5da-607f38d15341-c000.snappy.parquet\n",
    "-rw-r--r--  1 user  staff  3925441 Mar  2 14:45 part-00002-8c83f3b8-5e89-4fd3-b5da-607f38d15341-c000.snappy.parquet\n",
    "-rw-r--r--  1 user  staff  3923879 Mar  2 14:45 part-00003-8c83f3b8-5e89-4fd3-b5da-607f38d15341-c000.snappy.parquet\n",
    "-rw-r--r--  1 user  staff  3926901 Mar  2 14:45 part-00004-8c83f3b8-5e89-4fd3-b5da-607f38d15341-c000.snappy.parquet\n",
    "-rw-r--r--  1 user  staff  3925450 Mar  2 14:45 part-00005-8c83f3b8-5e89-4fd3-b5da-607f38d15341-c000.snappy.parquet\n",
    "-rw-r--r--  1 user  staff  3926899 Mar  2 14:45 part-00006-8c83f3b8-5e89-4fd3-b5da-607f38d15341-c000.snappy.parquet\n",
    "-rw-r--r--  1 user  staff  3927014 Mar  2 14:45 part-00007-8c83f3b8-5e89-4fd3-b5da-607f38d15341-c000.snappy.parquet\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|  128893|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df.registerTempTable('yellow_tripdata_2024_10')\n",
    "\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT count(1)\n",
    "FROM \n",
    "    yellow_tripdata_2024_10\n",
    "          \n",
    "WHERE\n",
    "    to_date(tpep_pickup_datetime) = '2024-10-15'\n",
    "\"\"\").show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128893"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "df \\\n",
    "    .withColumn('tpep_pickup_datetime', F.to_date(df.tpep_pickup_datetime)) \\\n",
    "    .filter(\"tpep_pickup_datetime = '2024-10-15'\") \\\n",
    "    .count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|datetime           |\n",
      "+-------------------+\n",
      "|2012-11-20 12:39:37|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.conf.set('spark.sql.session.timeZone', 'America/New_York')\n",
    "spark.sql(\"\"\"select to_timestamp('2012-11-20T17:39:37Z', \"yyyy-MM-dd'T'HH:mm:ssz\") as datetime\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+\n",
      "|pickup_date|     max(duration)|\n",
      "+-----------+------------------+\n",
      "| 2024-10-16| 162.6177777777778|\n",
      "| 2024-10-03|           143.325|\n",
      "| 2024-10-22|137.76055555555556|\n",
      "| 2024-10-18|114.83472222222221|\n",
      "| 2024-10-21| 89.89833333333333|\n",
      "+-----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df \\\n",
    "    .withColumn('duration', (df.tpep_dropoff_datetime - df.tpep_pickup_datetime).cast('long')/60/60) \\\n",
    "    .withColumn('pickup_date', F.to_date(df.tpep_pickup_datetime)) \\\n",
    "    .groupBy('pickup_date') \\\n",
    "        .max('duration') \\\n",
    "    .orderBy('max(duration)', ascending=False) \\\n",
    "    .limit(5) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11130.40s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-03-02 16:20:53--  https://d37ci6vzurychx.cloudfront.net/misc/taxi_zone_lookup.csv\n",
      "Resolving d37ci6vzurychx.cloudfront.net (d37ci6vzurychx.cloudfront.net)... 2600:9000:2795:600:b:20a5:b140:21, 2600:9000:2795:5a00:b:20a5:b140:21, 2600:9000:2795:1000:b:20a5:b140:21, ...\n",
      "Connecting to d37ci6vzurychx.cloudfront.net (d37ci6vzurychx.cloudfront.net)|2600:9000:2795:600:b:20a5:b140:21|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 12331 (12K) [text/csv]\n",
      "Saving to: ‘taxi_zone_lookup.csv.1’\n",
      "\n",
      "taxi_zone_lookup.cs 100%[===================>]  12,04K  --.-KB/s    in 0s      \n",
      "\n",
      "2025-03-02 16:20:53 (470 MB/s) - ‘taxi_zone_lookup.csv.1’ saved [12331/12331]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://d37ci6vzurychx.cloudfront.net/misc/taxi_zone_lookup.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zones = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv('taxi_zone_lookup.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zones.columns\n",
    "\n",
    "df_zones.registerTempTable('zones')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 104:============================>                            (4 + 4) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|                Zone|count(1)|\n",
      "+--------------------+--------+\n",
      "|Governor's Island...|       1|\n",
      "|       Rikers Island|       2|\n",
      "|       Arden Heights|       2|\n",
      "|         Jamaica Bay|       3|\n",
      "| Green-Wood Cemetery|       3|\n",
      "+--------------------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "    pul.Zone,\n",
    "    COUNT(1)\n",
    "FROM \n",
    "    yellow_tripdata_2024_10 ylw_tax \n",
    "LEFT JOIN zones pul ON ylw_tax.PULocationID = pul.LocationID\n",
    "WHERE    to_date(ylw_tax.tpep_pickup_datetime) between  '2024-10-01' and  '2024-10-31' \n",
    "GROUP BY \n",
    "    1\n",
    "ORDER BY\n",
    "    2 ASC\n",
    "LIMIT 5;\n",
    "\"\"\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
